---
title: "From 57% to 99%: The Live Classification Marathon"
description: "A single day's journey through calibration failure, data re-recording, training methodology mismatches and 15 GUI iterations to achieve 99% average confidence across all seven fault types in live testing."
date: 2026-02-18
categories: [Development]
tags: [xgboost, fault-detection, probability-calibration, data-quality, gui-development, live-testing, digital-twin]
pin: false
image:
  path: assets/img/Confidence Dilemma.png
  alt: XGBoost confidence distribution showing the gap between offline accuracy and live prediction confidence
---

Today was the longest and most intensive development session of the entire project. What started as a straightforward attempt to improve confidence scores spiralled into a complete data re-recording campaign, a training methodology overhaul and 15 iterative GUI versions before achieving consistent 99% classification confidence across all seven fault types in live testing. This post documents every problem encountered, every failed approach and every lesson learned.

## The Confidence Problem

The previous session ended with a working XGBoost classifier achieving 99.7% accuracy on the 1400-scenario test set. Offline metrics were excellent. But live GUI testing told a different story: the model would correctly identify faults but report confidence scores as low as 57% and 68%, well below the 70% threshold required for a valid classification. A prediction of "Delayed_Deployment 57%" is technically correct but operationally useless since the operator cannot trust a result that the model itself seems uncertain about.

## Attempt 1: Probability Calibration (Failed)

The first instinct was to address the confidence scores directly using scikit-learn's `CalibratedClassifierCV`. This wraps the existing XGBoost model in a calibration layer that uses isotonic regression to remap raw probability outputs into better-calibrated confidence values. The theory was sound: XGBoost's internal probability estimates are based on leaf node statistics that can be poorly calibrated even when the underlying predictions are correct.

The calibration pipeline was integrated into the existing `merge_and_retrain_1400_v1.py` script with 5-fold internal cross-validation, running the same 10 random seeds as the uncalibrated baseline for direct comparison. The results were immediately telling: uncalibrated mean confidence was already 98.0% and the calibrated version actually dropped slightly. Calibration had no room to improve because the offline probabilities were already well-calibrated. The low live confidence scores were not a calibration problem.

This attempt took approximately two hours to implement and evaluate, only to confirm that the solution lay elsewhere. The calibrated model was reverted and the uncalibrated XGBoost was restored.

## Attempt 2: Feature Engineering (Failed)

With calibration ruled out, the next hypothesis was that the model needed more discriminative features to separate ambiguous fault classes. Analysis of the confusion matrix showed the weakest boundaries were between Asymmetric_Speed and Delayed_Deployment (87% and 94% per-class accuracy respectively). Eleven new features were engineered targeting these boundaries, including deployment rate ratios, speed deficit indicators and timing consistency metrics.

The V4 pipeline with 76 features (up from 65) was trained and evaluated. Results: Asymmetric held at 87.0%, Delayed actually dropped from 94.0% to 92.8% and overall accuracy dipped marginally from 97.0% to 96.9%. The new features had not moved the needle at all. This was the second failed approach in a single day.

## The Root Cause: Overlapping Training Data

Rather than trying a third algorithmic fix, it was time to examine the data itself. A detailed audit of the V1 batch recorder's scenario generation logic revealed the actual root cause.

For Delayed_Deployment scenarios, 80% (160 out of 200) were generated with one engine at the normal speed of 20.8 mm/s and the other engine at a slow speed between 3.0 and 10.0 mm/s. This is indistinguishable from Asymmetric_Speed, where one engine runs faster than the other. The remaining 20% had both engines slow but the dominant pattern created massive class overlap in the feature space. No amount of feature engineering could separate classes that were generated with identical parameter distributions.

This discovery validated an important principle that has been reinforced throughout this project: data quality fundamentally trumps algorithmic sophistication. The model was not confused because it lacked features. It was confused because the training data was ambiguous.

## V2 Batch Recorder: Clean Data Separation

A completely rewritten batch recorder was developed with strict parameter separation between fault classes. The key changes were:

For Delayed_Deployment, all 200 scenarios now have both engines at the same slow speed (3.0 to 18.0 mm/s range), ensuring a symmetrically slow deployment signature that is fundamentally different from the asymmetric one-fast-one-slow pattern of Asymmetric_Speed. For Asymmetric_Speed, all scenarios maintain a strict speed differential where one engine operates at normal speed (20.8 mm/s) while the other operates at a reduced speed, preserving the defining characteristic of the fault.

Every scenario uses unique randomised parameter values at 0.01 mm increments, with proper engine alternation patterns (E1-only, E2-only, and both-engine variants) distributed across each fault class.

## Recording 1000 Scenarios (With NX Lag Issues)

Recording 1000 new scenarios through NX MCD introduced practical challenges. The digital twin's physics simulation occasionally lagged behind the PLC commands, causing some recordings to complete before the deployment cycle had fully played out. These appeared as CSV files with fewer than the expected number of samples.

An integrated retry mechanism was built directly into the recorder script. `MAX_RECORDING_TIME` was increased from 30 to 45 seconds to accommodate slower simulations and a retry phase with up to 3 attempts per failed scenario was added after the main recording loop. A 5-second delay between retries gave NX time to reset properly. One particularly stubborn scenario (Asymmetric_Speed_0371) required a dedicated single-scenario recorder with a 60-second timeout and 5 retry attempts before it finally captured clean data.

The full recording session completed with all 1000 scenarios successfully captured, despite approximately 6 scenarios requiring retries due to NX lag.

## First Retraining: 99.7% but Oscillating and Stall Still Broken

The 1000 new V2 recordings were merged with the existing 400 Oscillating and Stall scenarios and retrained. Results jumped dramatically:

| Metric | Previous (V1 Data) | New (V2 Data) |
|--------|---------------------|---------------|
| Overall Accuracy | 97.0% ± 0.7% | 99.7% ± 0.3% |
| Asymmetric_Speed | 87.0% | 100.0% |
| Delayed_Deployment | 94.0% | 99.5% |

The five original fault classes were now essentially perfect. However, live GUI testing revealed that Oscillating_Deployment and Stall_Deployment were still misclassifying: oscillating faults were identified as Normal_Deployment, and stall faults were identified as Incomplete_Deployment.

## Recording Methodology Mismatch

The issue was not with the model but with how the 400 Oscillating and Stall scenarios had been recorded. The V1 400 recorder used a different recording methodology from the V2 1000 recorder.

The V2 recorder detected deployment start by monitoring Engine 1's position sensor crossing a threshold, then captured data for the full cycle duration. The V1 recorder used the PLC's `Recording_Start` tag and began recording immediately when parameters were set, before the transcowl had started moving. This meant the V1 recordings had a different temporal structure (long pre-deployment period at zero velocity) and when these recordings were used to train the model, the features extracted did not match what the live GUI produced during real deployments.

A V2-compatible 400 recorder was built, replicating the exact position-based cycle detection and recording methodology. All 400 Oscillating and Stall scenarios were re-recorded. The subsequent retraining achieved 99.8% ± 0.3% accuracy with all seven classes stable, including Oscillating at 99.8% and Stall at 99.5%.

## The GUI Misclassification Marathon (V10 to V15)

With the model performing excellently on the test set, live GUI testing began in earnest. The five static fault types (Normal, Asymmetric, Delayed, Incomplete, Combined) all classified correctly with 100% confidence immediately. But the two dynamic fault types, Oscillating_Deployment and Stall_Deployment, continued to misclassify in the GUI despite perfect offline metrics.

The root cause was a gap between how the GUI injected faults during live operation and how the batch recorder had generated training data. Bridging this gap required 15 iterative GUI versions over approximately four hours of systematic debugging. Each version tested a specific hypothesis, and each failure provided diagnostic information that narrowed the search.

### Stall_Deployment: The speed_deficit Threshold

Stall_Deployment proved the most persistent misclassification. The training data generated both engines at the same speed (randomly selected between 10.0 and 20.8 mm/s), then froze the affected engine's transcowl mid-deployment. The GUI's injection function initially used a fixed 15.0 mm/s for the affected engine and 20.8 mm/s for the unaffected engine, creating a velocity asymmetry that was not present in the training data.

Analysis of the feature extraction code revealed a critical threshold: `speed_deficit = 20.8 - min(avg_velocity)`. When speed_deficit exceeded 5.0 and position_deficit exceeded 4.0, a binary `speed_and_position_fault` feature triggered, pushing the classification toward Combined_Fault. At 15.0 mm/s, speed_deficit equals 5.8, which crosses the threshold. The model was not wrong; it was responding to a genuine Combined_Fault signature that the injection method was inadvertently creating.

| GUI Version | Affected Engine Speed | Non-affected Speed | Result | Why It Failed |
|-------------|----------------------|-------------------|--------|---------------|
| V10 | 15.0 mm/s | 15.0 mm/s | Incomplete | Both slow, position-only signature |
| V11 | 15.0 mm/s | 20.8 mm/s | Combined | speed_deficit = 5.8 crosses 5.0 threshold |
| V12 | 20.8 mm/s | 20.8 mm/s | Incomplete | Identical velocity profile to Incomplete |
| V14 | 20.0 mm/s | 20.0 mm/s | Stall 100% | Matches training pattern, speed_deficit = 0.8 |

The solution was to set both engines to 20.0 mm/s (matching the training data pattern of identical speeds), keeping speed_deficit at 0.8 (well below the 5.0 threshold) while letting the stall_ratio feature from the velocity-to-zero transition drive the classification.

### Oscillating_Deployment: Position Sensor Alignment

Oscillating_Deployment for Engine 2 consistently showed lower confidence than Engine 1 (52% vs 99%). The issue was timing: the oscillation thread waited for the affected engine's position sensor to exceed 5 mm before starting speed toggles, but the monitor loop triggered recording based on Engine 1's position sensor crossing the threshold. When oscillating Engine 2, E1 starts moving first (PLC processes sequentially), so recording began before E2 had reached 5 mm, and the first oscillation cycles were missed in the captured data.

The fix was to always use Engine 1's position sensor for deployment detection in the oscillation function, regardless of which engine was being oscillated. The number of oscillation cycles was also increased from 3 to 4 at a faster 0.8-second period (down from 1.0 seconds) to produce more velocity sign changes within the recording window.

## Sound System Development

Alongside the classification fixes, the GUI's aircraft alert sound system was developed and refined. Initial implementations used simple sine wave tones that sounded unrealistic. The final version generates aviation-authentic alert sounds using programmatic WAV synthesis.

The stall warning uses a 780 Hz carrier tone with 25 Hz amplitude modulation, creating a buzzing rattle that approximates the stick shaker sound heard in real aircraft stall warnings. The combined fault alert uses a descending three-tone horn (1200, 900, 700 Hz) followed by a rapid 1600/1100 Hz warble, inspired by GPWS "PULL UP" warnings. A continuous engine rumble loop using an 80 Hz drone with 160 Hz and 240 Hz harmonics plus low-pass filtered noise provides ambient turbine sound throughout deployment cycles.

All alert volumes were maximised in the final version after testing revealed the generated sounds were too quiet even at full system volume.

## Final Results

The final GUI version (V15) achieved a perfect 10-out-of-10 classification run across all fault types:

| Fault Type | Engine | Confidence | Severity |
|------------|--------|------------|----------|
| Normal | Both | 100% | OK |
| Asymmetric_Speed | E1 | 100% | CAUT |
| Asymmetric_Speed | E2 | 100% | CAUT |
| Delayed_Deployment | Both | 100% | WARN |
| Incomplete_Deployment | Both | 100% | WARN |
| Combined_Fault | Both | 100% | WARN |
| Oscillating_Deployment | E1 | 99% | WARN |
| Oscillating_Deployment | E2 | 98% | WARN |
| Stall_Deployment | E1 | 100% | WARN |
| Stall_Deployment | E2 | 99% | WARN |

Session statistics: 99% average confidence, 24.7 ms average latency, 90% fault rate across 10 deployment cycles.

{% include embed/video.html src='assets/video/Fault Detection Model (Compressed).mp4' title='Live fault detection demonstration showing all seven fault types classified in real-time' %}

## Lessons Learned

Today reinforced several principles that have been consistent themes throughout this project.

**Data quality is foundational.** Two hours of calibration work and another hour of feature engineering produced zero improvement because the underlying data had class overlap. Re-recording with clean parameter separation immediately jumped accuracy from 97% to 99.7%. The lesson is not that calibration and feature engineering are useless, but that they cannot compensate for fundamentally flawed training data.

**Methodology consistency between training and inference is critical.** The model achieved 99.8% offline accuracy but still misclassified in the GUI because the live fault injection methods did not exactly replicate the conditions under which the training data was recorded. Every difference in speed parameters, timing triggers or position sensor selection created a distribution shift that the model detected and responded to. The 15-version GUI debugging marathon was entirely about closing these gaps.

**Systematic debugging beats intuition.** Each GUI version tested a single, specific hypothesis informed by the previous failure's diagnostic data. The speed_deficit threshold discovery came from reading the feature extraction code line by line, not from guessing. The position sensor alignment fix came from tracing exactly which sensor triggered recording start in the monitor loop versus the oscillation thread. These are not insights that come from experimentation alone; they require understanding the system at a code level.

## What is Next

The immediate next step is rebuilding the operator interface in PyQt5. The current GUI uses CustomTkinter, which served well for rapid prototyping but has limitations in layout flexibility and visual polish. PyQt5 offers dockable panels, stylesheet-based theming, and smoother real-time charting. Both versions will be run side by side to compare performance, appearance and maintainability, with the outcome documented in a formal technical decision. Beyond the framework comparison, planned additions include a real-time 2D thrust reverser animation, a multi-cycle predictive trend dashboard and a system health scoring algorithm.
